
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>32. Intel Virtual Function Driver &#8212; Data Plane Development Kit 23.11.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="33. IONIC Driver" href="ionic.html" />
    <link rel="prev" title="31. IGC Poll Mode Driver" href="igc.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="intel-virtual-function-driver">
<h1><span class="section-number">32. </span>Intel Virtual Function Driver</h1>
<p>Supported Intel® Ethernet Controllers (see the <em>DPDK Release Notes</em> for details)
support the following modes of operation in a virtualized environment:</p>
<ul class="simple">
<li><p><strong>SR-IOV mode</strong>: Involves direct assignment of part of the port resources to different guest operating systems
using the PCI-SIG Single Root I/O Virtualization (SR IOV) standard,
also known as “native mode” or “pass-through” mode.
In this chapter, this mode is referred to as IOV mode.</p></li>
<li><p><strong>VMDq mode</strong>: Involves central management of the networking resources by an IO Virtual Machine (IOVM) or
a Virtual Machine Monitor (VMM), also known as software switch acceleration mode.
In this chapter, this mode is referred to as the Next Generation VMDq mode.</p></li>
</ul>
<section id="sr-iov-mode-utilization-in-a-dpdk-environment">
<h2><span class="section-number">32.1. </span>SR-IOV Mode Utilization in a DPDK Environment</h2>
<p>The DPDK uses the SR-IOV feature for hardware-based I/O sharing in IOV mode.
Therefore, it is possible to partition SR-IOV capability on Ethernet controller NIC resources logically and
expose them to a virtual machine as a separate PCI function called a “Virtual Function”.
Refer to <a class="reference internal" href="#figure-single-port-nic"><span class="std std-numref">Fig. 32.13</span></a>.</p>
<p>Therefore, a NIC is logically distributed among multiple virtual machines (as shown in <a class="reference internal" href="#figure-single-port-nic"><span class="std std-numref">Fig. 32.13</span></a>),
while still having global data in common to share with the Physical Function and other Virtual Functions.
The DPDK fm10kvf, iavf, igbvf or ixgbevf as a Poll Mode Driver (PMD) serves for the Intel® 82576 Gigabit Ethernet Controller,
Intel® Ethernet Controller I350 family, Intel® 82599 10 Gigabit Ethernet Controller NIC,
Intel® Fortville 10/40 Gigabit Ethernet Controller NIC’s virtual PCI function, or PCIe host-interface of the Intel Ethernet Switch
FM10000 Series.
Meanwhile the DPDK Poll Mode Driver (PMD) also supports “Physical Function” of such NIC’s on the host.</p>
<p>The DPDK PF/VF Poll Mode Driver (PMD) supports the Layer 2 switch on Intel® 82576 Gigabit Ethernet Controller,
Intel® Ethernet Controller I350 family, Intel® 82599 10 Gigabit Ethernet Controller,
and Intel® Fortville 10/40 Gigabit Ethernet Controller NICs so that guest can choose it for inter virtual machine traffic in SR-IOV mode.</p>
<p>For more detail on SR-IOV, please refer to the following documents:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.intel.com/network/connectivity/solutions/vmdc.htm">SR-IOV provides hardware based I/O sharing</a></p></li>
<li><p><a class="reference external" href="http://www.intel.com/content/www/us/en/pci-express/pci-sig-single-root-io-virtualization-support-in-virtualization-technology-for-connectivity-paper.html">PCI-SIG-Single Root I/O Virtualization Support on IA</a></p></li>
<li><p><a class="reference external" href="http://www.intel.com/content/www/us/en/virtualization/server-virtualization/scalable-i-o-virtualized-servers-paper.html">Scalable I/O Virtualized Servers</a></p></li>
</ul>
<figure class="align-default" id="id1">
<span id="figure-single-port-nic"></span><img alt="../_images/single_port_nic.png" src="../_images/single_port_nic.png" />
<figcaption>
<p><span class="caption-number">Fig. 32.13 </span><span class="caption-text">Virtualization for a Single Port NIC in SR-IOV Mode</span></p>
</figcaption>
</figure>
<section id="physical-and-virtual-function-infrastructure">
<h3><span class="section-number">32.1.1. </span>Physical and Virtual Function Infrastructure</h3>
<p>The following describes the Physical Function and Virtual Functions infrastructure for the supported Ethernet Controller NICs.</p>
<p>Virtual Functions operate under the respective Physical Function on the same NIC Port and therefore have no access
to the global NIC resources that are shared between other functions for the same NIC port.</p>
<p>A Virtual Function has basic access to the queue resources and control structures of the queues assigned to it.
For global resource access, a Virtual Function has to send a request to the Physical Function for that port,
and the Physical Function operates on the global resources on behalf of the Virtual Function.
For this out-of-band communication, an SR-IOV enabled NIC provides a memory buffer for each Virtual Function,
which is called a “Mailbox”.</p>
<section id="intel-ethernet-adaptive-virtual-function">
<h4><span class="section-number">32.1.1.1. </span>Intel® Ethernet Adaptive Virtual Function</h4>
<p>Adaptive Virtual Function (IAVF) is a SR-IOV Virtual Function with the same device id (8086:1889) on different Intel Ethernet Controller.
IAVF Driver is VF driver which supports for all future Intel devices without requiring a VM update. And since this happens to be an adaptive VF driver,
every new drop of the VF driver would add more and more advanced features that can be turned on in the VM if the underlying HW device supports those
advanced features based on a device agnostic way without ever compromising on the base functionality. IAVF provides generic hardware interface and
interface between IAVF driver and a compliant PF driver is specified.</p>
<p>Intel products starting Ethernet Controller 700 Series to support Adaptive Virtual Function.</p>
<p>The way to generate Virtual Function is like normal, and the resource of VF assignment depends on the NIC Infrastructure.</p>
<p>For more detail on SR-IOV, please refer to the following documents:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.intel.com/content/dam/www/public/us/en/documents/product-specifications/ethernet-adaptive-virtual-function-hardware-spec.pdf">Intel® IAVF HAS</a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use DPDK IAVF PMD on Intel® 700 Series Ethernet Controller, the device id (0x1889) need to specified during device
assignment in hypervisor. Take qemu for example, the device assignment should carry the IAVF device id (0x1889) like
<code class="docutils literal notranslate"><span class="pre">-device</span> <span class="pre">vfio-pci,x-pci-device-id=0x1889,host=03:0a.0</span></code>.</p>
<p>When IAVF is backed by an Intel® E810 device, the “Protocol Extraction” feature which is supported by ice PMD is also
available for IAVF PMD. The same devargs with the same parameters can be applied to IAVF PMD, for detail please reference
the section <code class="docutils literal notranslate"><span class="pre">Protocol</span> <span class="pre">extraction</span> <span class="pre">for</span> <span class="pre">per</span> <span class="pre">queue</span></code> of ice.rst.</p>
<p>Quanta size configuration is also supported when IAVF is backed by an Intel® E810 device by setting <code class="docutils literal notranslate"><span class="pre">devargs</span></code>
parameter <code class="docutils literal notranslate"><span class="pre">quanta_size</span></code> like <code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">18:00.0,quanta_size=2048</span></code>. The default value is 1024, and quanta size should be
set as the product of 64 in legacy host interface mode.</p>
<p>When IAVF is backed by an Intel® E810 device or an Intel® 700 Series Ethernet device, the reset watchdog is enabled
when link state changes to down. The default period is 2000us, defined by <code class="docutils literal notranslate"><span class="pre">IAVF_DEV_WATCHDOG_PERIOD</span></code>.
Set <code class="docutils literal notranslate"><span class="pre">devargs</span></code> parameter <code class="docutils literal notranslate"><span class="pre">watchdog_period</span></code> to adjust the watchdog period in microseconds, or set it to 0 to disable the watchdog,
for example, <code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">18:01.0,watchdog_period=5000</span></code> or <code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">18:01.0,watchdog_period=0</span></code>.</p>
<p>Enable VF auto-reset by setting the devargs parameter like <code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">18:01.0,auto_reset=1</span></code>
when IAVF is backed by an Intel® E810 device
or an Intel® 700 Series Ethernet device.</p>
<p>Stop polling Rx/Tx hardware queue when link is down
by setting the <code class="docutils literal notranslate"><span class="pre">devargs</span></code> parameter like <code class="docutils literal notranslate"><span class="pre">-a</span> <span class="pre">18:01.0,no-poll-on-link-down=1</span></code>
when IAVF is backed by an Intel® E810 device or an Intel® 700 Series Ethernet device.</p>
</div>
</section>
<section id="the-pcie-host-interface-of-intel-ethernet-switch-fm10000-series-vf-infrastructure">
<h4><span class="section-number">32.1.1.2. </span>The PCIE host-interface of Intel Ethernet Switch FM10000 Series VF infrastructure</h4>
<p>In a virtualized environment, the programmer can enable a maximum of <em>64 Virtual Functions (VF)</em>
globally per PCIE host-interface of the Intel Ethernet Switch FM10000 Series device.
Each VF can have a maximum of 16 queue pairs.
The Physical Function in host could be only configured by the Linux* fm10k driver
(in the case of the Linux Kernel-based Virtual Machine [KVM]), DPDK PMD PF driver doesn’t support it yet.</p>
<p>For example,</p>
<ul>
<li><p>Using Linux* fm10k driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">rmmod fm10k (To remove the fm10k module)</span>
<span class="go">insmod fm0k.ko max_vfs=2,2 (To enable two Virtual Functions per port)</span>
</pre></div>
</div>
</li>
</ul>
<p>Virtual Function enumeration is performed in the following sequence by the Linux* pci driver for a dual-port NIC.
When you enable the four Virtual Functions with the above command, the four enabled functions have a Function#
represented by (Bus#, Device#, Function#) in sequence starting from 0 to 3.
However:</p>
<ul class="simple">
<li><p>Virtual Functions 0 and 2 belong to Physical Function 0</p></li>
<li><p>Virtual Functions 1 and 3 belong to Physical Function 1</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above is an important consideration to take into account when targeting specific packets to a selected port.</p>
</div>
</section>
<section id="intel-x710-xl710-gigabit-ethernet-controller-vf-infrastructure">
<h4><span class="section-number">32.1.1.3. </span>Intel® X710/XL710 Gigabit Ethernet Controller VF Infrastructure</h4>
<p>In a virtualized environment, the programmer can enable a maximum of <em>128 Virtual Functions (VF)</em>
globally per Intel® X710/XL710 Gigabit Ethernet Controller NIC device.
The Physical Function in host could be either configured by the Linux* i40e driver
(in the case of the Linux Kernel-based Virtual Machine [KVM]) or by DPDK PMD PF driver.
When using both DPDK PMD PF/VF drivers, the whole NIC will be taken over by DPDK based application.</p>
<p>For example,</p>
<ul>
<li><p>Using Linux* i40e  driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">rmmod i40e (To remove the i40e module)</span>
<span class="go">insmod i40e.ko max_vfs=2,2 (To enable two Virtual Functions per port)</span>
</pre></div>
</div>
</li>
<li><p>Using the DPDK PMD PF i40e driver, bind the PF device to <code class="docutils literal notranslate"><span class="pre">vfio_pci</span></code> or <code class="docutils literal notranslate"><span class="pre">igb_uio</span></code> and
create VF devices. See <a class="reference internal" href="../linux_gsg/linux_drivers.html#linux-gsg-binding-kernel"><span class="std std-ref">Binding and Unbinding Network Ports to/from the Kernel Modules</span></a>.</p>
<p>Launch the DPDK testpmd/example or your own host daemon application using the DPDK PMD library.</p>
</li>
</ul>
<p>Virtual Function enumeration is performed in the following sequence by the Linux* pci driver for a dual-port NIC.
When you enable the four Virtual Functions with the above command, the four enabled functions have a Function#
represented by (Bus#, Device#, Function#) in sequence starting from 0 to 3.
However:</p>
<ul class="simple">
<li><p>Virtual Functions 0 and 2 belong to Physical Function 0</p></li>
<li><p>Virtual Functions 1 and 3 belong to Physical Function 1</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above is an important consideration to take into account when targeting specific packets to a selected port.</p>
<p>For Intel® X710/XL710 Gigabit Ethernet Controller, queues are in pairs. One queue pair means one receive queue and
one transmit queue. The default number of queue pairs per VF is 4, and can be 16 in maximum.</p>
</div>
</section>
<section id="intel-82599-10-gigabit-ethernet-controller-vf-infrastructure">
<h4><span class="section-number">32.1.1.4. </span>Intel® 82599 10 Gigabit Ethernet Controller VF Infrastructure</h4>
<p>The programmer can enable a maximum of <em>63 Virtual Functions</em> and there must be <em>one Physical Function</em> per Intel® 82599
10 Gigabit Ethernet Controller NIC port.
The reason for this is that the device allows for a maximum of 128 queues per port and a virtual/physical function has to
have at least one queue pair (RX/TX).
The current implementation of the DPDK ixgbevf driver supports a single queue pair (RX/TX) per Virtual Function.
The Physical Function in host could be either configured by the Linux* ixgbe driver
(in the case of the Linux Kernel-based Virtual Machine [KVM]) or by DPDK PMD PF driver.
When using both DPDK PMD PF/VF drivers, the whole NIC will be taken over by DPDK based application.</p>
<p>For example,</p>
<ul>
<li><p>Using Linux* ixgbe driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">rmmod ixgbe (To remove the ixgbe module)</span>
<span class="go">insmod ixgbe max_vfs=2,2 (To enable two Virtual Functions per port)</span>
</pre></div>
</div>
</li>
<li><p>Using the DPDK PMD PF ixgbe driver, bind the PF device to <code class="docutils literal notranslate"><span class="pre">vfio_pci</span></code> or <code class="docutils literal notranslate"><span class="pre">igb_uio</span></code> and
create VF devices. See <a class="reference internal" href="../linux_gsg/linux_drivers.html#linux-gsg-binding-kernel"><span class="std std-ref">Binding and Unbinding Network Ports to/from the Kernel Modules</span></a>.</p>
<p>Launch the DPDK testpmd/example or your own host daemon application using the DPDK PMD library.</p>
</li>
<li><p>Using the DPDK PMD PF ixgbe driver to enable VF RSS:</p>
<p>Same steps as above to bind the PF device, create VF devices, and
launch the DPDK testpmd/example or your own host daemon application using the DPDK PMD library.</p>
<p>The available queue number (at most 4) per VF depends on the total number of pool, which is
determined by the max number of VF at PF initialization stage and the number of queue specified
in config:</p>
<ul>
<li><p>If the max number of VFs (max_vfs) is set in the range of 1 to 32:</p>
<p>If the number of Rx queues is specified as 4 (<code class="docutils literal notranslate"><span class="pre">--rxq=4</span></code> in testpmd), then there are totally 32
pools (RTE_ETH_32_POOLS), and each VF could have 4 Rx queues;</p>
<p>If the number of Rx queues is specified as 2 (<code class="docutils literal notranslate"><span class="pre">--rxq=2</span></code> in testpmd), then there are totally 32
pools (RTE_ETH_32_POOLS), and each VF could have 2 Rx queues;</p>
</li>
<li><p>If the max number of VFs (max_vfs) is in the range of 33 to 64:</p>
<p>If the number of Rx queues in specified as 4 (<code class="docutils literal notranslate"><span class="pre">--rxq=4</span></code> in testpmd), then error message is expected
as <code class="docutils literal notranslate"><span class="pre">rxq</span></code> is not correct at this case;</p>
<p>If the number of rxq is 2 (<code class="docutils literal notranslate"><span class="pre">--rxq=2</span></code> in testpmd), then there is totally 64 pools (RTE_ETH_64_POOLS),
and each VF have 2 Rx queues;</p>
</li>
</ul>
<p>On host, to enable VF RSS functionality, rx mq mode should be set as RTE_ETH_MQ_RX_VMDQ_RSS
or RTE_ETH_MQ_RX_RSS mode, and SRIOV mode should be activated (max_vfs &gt;= 1).
It also needs config VF RSS information like hash function, RSS key, RSS key length.</p>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The limitation for VF RSS on Intel® 82599 10 Gigabit Ethernet Controller is:
The hash and key are shared among PF and all VF, the RETA table with 128 entries is also shared
among PF and all VF; So it could not to provide a method to query the hash and reta content per
VF on guest, while, if possible, please query them on host for the shared RETA information.</p>
</div>
<p>Virtual Function enumeration is performed in the following sequence by the Linux* pci driver for a dual-port NIC.
When you enable the four Virtual Functions with the above command, the four enabled functions have a Function#
represented by (Bus#, Device#, Function#) in sequence starting from 0 to 3.
However:</p>
<ul class="simple">
<li><p>Virtual Functions 0 and 2 belong to Physical Function 0</p></li>
<li><p>Virtual Functions 1 and 3 belong to Physical Function 1</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above is an important consideration to take into account when targeting specific packets to a selected port.</p>
</div>
</section>
<section id="intel-82576-gigabit-ethernet-controller-and-intel-ethernet-controller-i350-family-vf-infrastructure">
<h4><span class="section-number">32.1.1.5. </span>Intel® 82576 Gigabit Ethernet Controller and Intel® Ethernet Controller I350 Family VF Infrastructure</h4>
<p>In a virtualized environment, an Intel® 82576 Gigabit Ethernet Controller serves up to eight virtual machines (VMs).
The controller has 16 TX and 16 RX queues.
They are generally referred to (or thought of) as queue pairs (one TX and one RX queue).
This gives the controller 16 queue pairs.</p>
<p>A pool is a group of queue pairs for assignment to the same VF, used for transmit and receive operations.
The controller has eight pools, with each pool containing two queue pairs, that is, two TX and two RX queues assigned to each VF.</p>
<p>In a virtualized environment, an Intel® Ethernet Controller I350 family device serves up to eight virtual machines (VMs) per port.
The eight queues can be accessed by eight different VMs if configured correctly (the i350 has 4x1GbE ports each with 8T X and 8 RX queues),
that means, one Transmit and one Receive queue assigned to each VF.</p>
<p>For example,</p>
<ul>
<li><p>Using Linux* igb driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">rmmod igb (To remove the igb module)</span>
<span class="go">insmod igb max_vfs=2,2 (To enable two Virtual Functions per port)</span>
</pre></div>
</div>
</li>
<li><p>Using the DPDK PMD PF igb driver, bind the PF device to <code class="docutils literal notranslate"><span class="pre">vfio_pci</span></code> or <code class="docutils literal notranslate"><span class="pre">igb_uio</span></code> and
create VF devices. See <a class="reference internal" href="../linux_gsg/linux_drivers.html#linux-gsg-binding-kernel"><span class="std std-ref">Binding and Unbinding Network Ports to/from the Kernel Modules</span></a>.</p>
<p>Launch DPDK testpmd/example or your own host daemon application using the DPDK PMD library.</p>
</li>
</ul>
<p>Virtual Function enumeration is performed in the following sequence by the Linux* pci driver for a four-port NIC.
When you enable the four Virtual Functions with the above command, the four enabled functions have a Function#
represented by (Bus#, Device#, Function#) in sequence, starting from 0 to 7.
However:</p>
<ul class="simple">
<li><p>Virtual Functions 0 and 4 belong to Physical Function 0</p></li>
<li><p>Virtual Functions 1 and 5 belong to Physical Function 1</p></li>
<li><p>Virtual Functions 2 and 6 belong to Physical Function 2</p></li>
<li><p>Virtual Functions 3 and 7 belong to Physical Function 3</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above is an important consideration to take into account when targeting specific packets to a selected port.</p>
</div>
</section>
</section>
<section id="validated-hypervisors">
<h3><span class="section-number">32.1.2. </span>Validated Hypervisors</h3>
<p>The validated hypervisor is:</p>
<ul class="simple">
<li><p>KVM (Kernel Virtual Machine) with  Qemu, version 0.14.0</p></li>
</ul>
<p>However, the hypervisor is bypassed to configure the Virtual Function devices using the Mailbox interface,
the solution is hypervisor-agnostic.
Xen* and VMware* (when SR- IOV is supported) will also be able to support the DPDK with Virtual Function driver support.</p>
</section>
<section id="expected-guest-operating-system-in-virtual-machine">
<h3><span class="section-number">32.1.3. </span>Expected Guest Operating System in Virtual Machine</h3>
<p>The expected guest operating systems in a virtualized environment are:</p>
<ul class="simple">
<li><p>Fedora* 14 (64-bit)</p></li>
<li><p>Ubuntu* 10.04 (64-bit)</p></li>
</ul>
<p>For supported kernel versions, refer to the <em>DPDK Release Notes</em>.</p>
</section>
</section>
<section id="setting-up-a-kvm-virtual-machine-monitor">
<span id="intel-vf-kvm"></span><h2><span class="section-number">32.2. </span>Setting Up a KVM Virtual Machine Monitor</h2>
<p>The following describes a target environment:</p>
<ul class="simple">
<li><p>Host Operating System: Fedora 14</p></li>
<li><p>Hypervisor: KVM (Kernel Virtual Machine) with Qemu  version 0.14.0</p></li>
<li><p>Guest Operating System: Fedora 14</p></li>
<li><p>Linux Kernel Version: Refer to the  <em>DPDK Getting Started Guide</em></p></li>
<li><p>Target Applications:  l2fwd, l3fwd-vf</p></li>
</ul>
<p>The setup procedure is as follows:</p>
<ol class="arabic">
<li><p>Before booting the Host OS, open <strong>BIOS setup</strong> and enable <strong>Intel® VT features</strong>.</p></li>
<li><p>While booting the Host OS kernel, pass the intel_iommu=on kernel command line argument using GRUB.
When using DPDK PF driver on host, pass the iommu=pt kernel command line argument in GRUB.</p></li>
<li><p>Download qemu-kvm-0.14.0 from
<a class="reference external" href="http://sourceforge.net/projects/kvm/files/qemu-kvm/">http://sourceforge.net/projects/kvm/files/qemu-kvm/</a>
and install it in the Host OS using the following steps:</p>
<p>When using a recent kernel (2.6.25+) with kvm modules included:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">tar xzf qemu-kvm-release.tar.gz</span>
<span class="go">cd qemu-kvm-release</span>
<span class="go">./configure --prefix=/usr/local/kvm</span>
<span class="go">make</span>
<span class="go">sudo make install</span>
<span class="go">sudo /sbin/modprobe kvm-intel</span>
</pre></div>
</div>
<p>When using an older kernel, or a kernel from a distribution without the kvm modules,
you must download (from the same link), compile and install the modules yourself:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">tar xjf kvm-kmod-release.tar.bz2</span>
<span class="go">cd kvm-kmod-release</span>
<span class="go">./configure</span>
<span class="go">make</span>
<span class="go">sudo make install</span>
<span class="go">sudo /sbin/modprobe kvm-intel</span>
</pre></div>
</div>
<p>qemu-kvm installs in the /usr/local/bin directory.</p>
<p>For more details about KVM configuration and usage, please refer to:</p>
<p><a class="reference external" href="http://www.linux-kvm.org/page/HOWTO1">http://www.linux-kvm.org/page/HOWTO1</a>.</p>
</li>
<li><p>Create a Virtual Machine and install Fedora 14 on the Virtual Machine.
This is referred to as the Guest Operating System (Guest OS).</p></li>
<li><p>Download and install the latest ixgbe driver from
<a class="reference external" href="https://downloadcenter.intel.com/download/14687">intel.com</a>.</p></li>
<li><p>In the Host OS</p>
<p>When using Linux kernel ixgbe driver, unload the Linux ixgbe driver and reload it with the max_vfs=2,2 argument:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">rmmod ixgbe</span>
<span class="go">modprobe ixgbe max_vfs=2,2</span>
</pre></div>
</div>
<p>When using DPDK PMD PF driver, bind the PF device to <code class="docutils literal notranslate"><span class="pre">vfio_pci</span></code> or <code class="docutils literal notranslate"><span class="pre">igb_uio</span></code> and
create VF devices. See <a class="reference internal" href="../linux_gsg/linux_drivers.html#linux-gsg-binding-kernel"><span class="std std-ref">Binding and Unbinding Network Ports to/from the Kernel Modules</span></a>.</p>
<p>Let say we have a machine with four physical ixgbe ports:</p>
<blockquote>
<div><p>0000:02:00.0</p>
<p>0000:02:00.1</p>
<p>0000:0e:00.0</p>
<p>0000:0e:00.1</p>
</div></blockquote>
<p>The mentioned steps above should result in two vfs for device 0000:02:00.0:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ls -alrt /sys/bus/pci/devices/0000\:02\:00.0/virt*</span>
<span class="go">lrwxrwxrwx. 1 root root 0 Apr 13 05:40 /sys/bus/pci/devices/0000:02:00.0/virtfn1 -&gt; ../0000:02:10.2</span>
<span class="go">lrwxrwxrwx. 1 root root 0 Apr 13 05:40 /sys/bus/pci/devices/0000:02:00.0/virtfn0 -&gt; ../0000:02:10.0</span>
</pre></div>
</div>
<p>It also creates two vfs for device 0000:02:00.1:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ls -alrt /sys/bus/pci/devices/0000\:02\:00.1/virt*</span>
<span class="go">lrwxrwxrwx. 1 root root 0 Apr 13 05:51 /sys/bus/pci/devices/0000:02:00.1/virtfn1 -&gt; ../0000:02:10.3</span>
<span class="go">lrwxrwxrwx. 1 root root 0 Apr 13 05:51 /sys/bus/pci/devices/0000:02:00.1/virtfn0 -&gt; ../0000:02:10.1</span>
</pre></div>
</div>
</li>
<li><p>List the PCI devices connected and notice that the Host OS shows two Physical Functions (traditional ports)
and four Virtual Functions (two for each port).
This is the result of the previous step.</p></li>
<li><p>Insert the pci_stub module to hold the PCI devices that are freed from the default driver using the following command
(see <a class="reference external" href="http://www.linux-kvm.org/page/How_to_assign_devices_with_VT-d_in_KVM">http://www.linux-kvm.org/page/How_to_assign_devices_with_VT-d_in_KVM</a> Section 4 for more information):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sudo /sbin/modprobe pci-stub</span>
</pre></div>
</div>
<p>Unbind the default driver from the PCI devices representing the Virtual Functions.
A script to perform this action is as follows:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">echo &quot;8086 10ed&quot; &gt; /sys/bus/pci/drivers/pci-stub/new_id</span>
<span class="go">echo 0000:08:10.0 &gt; /sys/bus/pci/devices/0000:08:10.0/driver/unbind</span>
<span class="go">echo 0000:08:10.0 &gt; /sys/bus/pci/drivers/pci-stub/bind</span>
</pre></div>
</div>
<p>where, 0000:08:10.0 belongs to the Virtual Function visible in the Host OS.</p>
</li>
<li><p>Now, start the Virtual Machine by running the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">/usr/local/kvm/bin/qemu-system-x86_64 -m 4096 -smp 4 -boot c -hda lucid.qcow2 -device pci-assign,host=08:10.0</span>
</pre></div>
</div>
<p>where:</p>
<blockquote>
<div><p>— -m = memory to assign</p>
<p class="attribution">—-smp = number of smp cores</p>
</div></blockquote>
<blockquote>
<div><p>— -boot = boot option</p>
<p class="attribution">—-hda = virtual disk image</p>
</div></blockquote>
<blockquote>
<div><p>— -device = device to attach</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>— The pci-assign,host=08:10.0 value indicates that you want to attach a PCI device
to a Virtual Machine and the respective (Bus:Device.Function)
numbers should be passed for the Virtual Function to be attached.</p>
<p>— qemu-kvm-0.14.0 allows a maximum of four PCI devices assigned to a VM,
but this is qemu-kvm version dependent since qemu-kvm-0.14.1 allows a maximum of five PCI devices.</p>
<p>— qemu-system-x86_64 also has a -cpu command line option that is used to select the cpu_model
to emulate in a Virtual Machine. Therefore, it can be used as:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">/usr/local/kvm/bin/qemu-system-x86_64 -cpu ?</span>

<span class="gp gp-VirtualEnv">(to list all available cpu_models)</span>

<span class="go">/usr/local/kvm/bin/qemu-system-x86_64 -m 4096 -cpu host -smp 4 -boot c -hda lucid.qcow2 -device pci-assign,host=08:10.0</span>

<span class="gp gp-VirtualEnv">(to use the same cpu_model equivalent to the host cpu)</span>
</pre></div>
</div>
<p>For more information, please refer to: <a class="reference external" href="http://wiki.qemu.org/Features/CPUModels">http://wiki.qemu.org/Features/CPUModels</a>.</p>
</div>
</li>
<li><p>If use vfio-pci to pass through device instead of pci-assign, steps 8 and 9 need to be updated to bind device to vfio-pci and
replace pci-assign with vfio-pci when start virtual machine.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sudo /sbin/modprobe vfio-pci</span>

<span class="go">echo &quot;8086 10ed&quot; &gt; /sys/bus/pci/drivers/vfio-pci/new_id</span>
<span class="go">echo 0000:08:10.0 &gt; /sys/bus/pci/devices/0000:08:10.0/driver/unbind</span>
<span class="go">echo 0000:08:10.0 &gt; /sys/bus/pci/drivers/vfio-pci/bind</span>

<span class="go">/usr/local/kvm/bin/qemu-system-x86_64 -m 4096 -smp 4 -boot c -hda lucid.qcow2 -device vfio-pci,host=08:10.0</span>
</pre></div>
</div>
</li>
<li><p>Install and run DPDK host app to take  over the Physical Function. Eg.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">./&lt;build_dir&gt;/app/dpdk-testpmd -l 0-3 -n 4 -- -i</span>
</pre></div>
</div>
</li>
<li><p>Finally, access the Guest OS using vncviewer with the localhost:5900 port and check the lspci command output in the Guest OS.
The virtual functions will be listed as available for use.</p></li>
<li><p>Configure and install the DPDK on the Guest OS as normal, that is, there is no change to the normal installation procedure.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are unable to compile the DPDK and you are getting “error: CPU you selected does not support x86-64 instruction set”,
power off the Guest OS and start the virtual machine with the correct -cpu option in the qemu- system-x86_64 command as shown in step 9.
You must select the best x86_64 cpu_model to emulate or you can select host option if available.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Run the DPDK l2fwd sample application in the Guest OS with Hugepages enabled.
For the expected benchmark performance, you must pin the cores from the Guest OS to the Host OS (taskset can be used to do this) and
you must also look at the PCI Bus layout on the board to ensure you are not running the traffic over the QPI Interface.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The Virtual Machine Manager (the Fedora package name is virt-manager) is a utility for virtual machine management
that can also be used to create, start, stop and delete virtual machines.
If this option is used, step 2 and 6 in the instructions provided will be different.</p></li>
<li><p>virsh, a command line utility for virtual machine management,
can also be used to bind and unbind devices to a virtual machine in Ubuntu.
If this option is used, step 6 in the instructions provided will be different.</p></li>
<li><p>The Virtual Machine Monitor (see <a class="reference internal" href="#figure-perf-benchmark"><span class="std std-numref">Fig. 32.14</span></a>) is equivalent to a Host OS with KVM installed as described in the instructions.</p></li>
</ul>
</div>
<figure class="align-default" id="id2">
<span id="figure-perf-benchmark"></span><img alt="../_images/perf_benchmark.png" src="../_images/perf_benchmark.png" />
<figcaption>
<p><span class="caption-number">Fig. 32.14 </span><span class="caption-text">Performance Benchmark Setup</span></p>
</figcaption>
</figure>
</section>
<section id="dpdk-sr-iov-pmd-pf-vf-driver-usage-model">
<h2><span class="section-number">32.3. </span>DPDK SR-IOV PMD PF/VF Driver Usage Model</h2>
<section id="fast-host-based-packet-processing">
<h3><span class="section-number">32.3.1. </span>Fast Host-based Packet Processing</h3>
<p>Software Defined Network (SDN) trends are demanding fast host-based packet handling.
In a virtualization environment,
the DPDK VF PMD performs the same throughput result as a non-VT native environment.</p>
<p>With such host instance fast packet processing, lots of services such as filtering, QoS,
DPI can be offloaded on the host fast path.</p>
<p><a class="reference internal" href="#figure-fast-pkt-proc"><span class="std std-numref">Fig. 32.15</span></a> shows the scenario where some VMs directly communicate externally via a VFs,
while others connect to a virtual switch and share the same uplink bandwidth.</p>
<figure class="align-default" id="id3">
<span id="figure-fast-pkt-proc"></span><img alt="../_images/fast_pkt_proc.png" src="../_images/fast_pkt_proc.png" />
<figcaption>
<p><span class="caption-number">Fig. 32.15 </span><span class="caption-text">Fast Host-based Packet Processing</span></p>
</figcaption>
</figure>
</section>
</section>
<section id="sr-iov-pf-vf-approach-for-inter-vm-communication">
<h2><span class="section-number">32.4. </span>SR-IOV (PF/VF) Approach for Inter-VM Communication</h2>
<p>Inter-VM data communication is one of the traffic bottle necks in virtualization platforms.
SR-IOV device assignment helps a VM to attach the real device, taking advantage of the bridge in the NIC.
So VF-to-VF traffic within the same physical port (VM0&lt;-&gt;VM1) have hardware acceleration.
However, when VF crosses physical ports (VM0&lt;-&gt;VM2), there is no such hardware bridge.
In this case, the DPDK PMD PF driver provides host forwarding between such VMs.</p>
<p><a class="reference internal" href="#figure-inter-vm-comms"><span class="std std-numref">Fig. 32.16</span></a> shows an example.
In this case an update of the MAC address lookup tables in both the NIC and host DPDK application is required.</p>
<p>In the NIC, writing the destination of a MAC address belongs to another cross device VM to the PF specific pool.
So when a packet comes in, its destination MAC address will match and forward to the host DPDK PMD application.</p>
<p>In the host DPDK application, the behavior is similar to L2 forwarding,
that is, the packet is forwarded to the correct PF pool.
The SR-IOV NIC switch forwards the packet to a specific VM according to the MAC destination address
which belongs to the destination VF on the VM.</p>
<figure class="align-default" id="id4">
<span id="figure-inter-vm-comms"></span><img alt="../_images/inter_vm_comms.png" src="../_images/inter_vm_comms.png" />
<figcaption>
<p><span class="caption-number">Fig. 32.16 </span><span class="caption-text">Inter-VM Communication</span></p>
</figcaption>
</figure>
</section>
<section id="windows-support">
<h2><span class="section-number">32.5. </span>Windows Support</h2>
<ul class="simple">
<li><p>IAVF PMD currently is supported only inside Windows guest created on Linux host.</p></li>
<li><p>Physical PCI resources are exposed as virtual functions
into Windows VM using SR-IOV pass-through feature.</p></li>
<li><p>Create a Windows guest on Linux host using KVM hypervisor.
Refer to the steps mentioned in the above section: <a class="reference internal" href="#intel-vf-kvm"><span class="std std-ref">Setting Up a KVM Virtual Machine Monitor</span></a>.</p></li>
<li><p>In the Host machine, download and install the kernel Ethernet driver
for <a class="reference external" href="https://downloadcenter.intel.com/download/24411">i40e</a>
or <a class="reference external" href="https://downloadcenter.intel.com/download/29746">ice</a>.</p></li>
<li><p>For Windows guest, install NetUIO driver
in place of existing built-in (inbox) Virtual Function driver.</p></li>
<li><p>To load NetUIO driver, follow the steps mentioned in <a class="reference external" href="https://git.dpdk.org/dpdk-kmods/tree/windows/netuio/README.rst">dpdk-kmods repository</a>.</p></li>
</ul>
</section>
<section id="inline-ipsec-support">
<h2><span class="section-number">32.6. </span>Inline IPsec Support</h2>
<ul class="simple">
<li><p>IAVF PMD supports inline crypto processing depending on the underlying
hardware crypto capabilities. IPsec Security Gateway Sample Application
supports inline IPsec processing for IAVF PMD. For more details see the
IPsec Security Gateway Sample Application and Security library
documentation.</p></li>
</ul>
</section>
<section id="limitations-or-knowing-issues">
<h2><span class="section-number">32.7. </span>Limitations or Knowing issues</h2>
<section id="byte-rx-descriptor-setting-is-not-available">
<h3><span class="section-number">32.7.1. </span>16 Byte RX Descriptor setting is not available</h3>
<p>Currently the VF’s RX descriptor size is decided by PF. There’s no PF-VF
interface for VF to request the RX descriptor size, also no interface to notify
VF its own RX descriptor size.
For all available versions of the kernel PF drivers, these drivers don’t
support 16 bytes RX descriptor. If the Linux kernel driver is used as host driver,
while DPDK iavf PMD is used as the VF driver, DPDK cannot choose 16 bytes receive
descriptor. The reason is that the RX descriptor is already set to 32 bytes by
the all existing kernel driver.
In the future, if the any kernel driver supports 16 bytes RX descriptor, user
should make sure the DPDK VF uses the same RX descriptor size.</p>
</section>
<section id="i40e-vf-performance-is-impacted-by-pci-extended-tag-setting">
<h3><span class="section-number">32.7.2. </span>i40e: VF performance is impacted by PCI extended tag setting</h3>
<p>To reach maximum NIC performance in the VF the PCI extended tag must be
enabled. But the kernel driver does not set this feature during initialization.
So when running traffic on a VF which is managed by the kernel PF driver, a
significant NIC performance downgrade has been observed (for 64 byte packets,
there is about 25% line-rate downgrade for a 25GbE device and about 35% for a
40GbE device).</p>
<p>For kernel version &gt;= 4.11, the kernel’s PCI driver will enable the extended
tag if it detects that the device supports it. So by default, this is not an
issue. For kernels &lt;= 4.11 or when the PCI extended tag is disabled it can be
enabled using the steps below.</p>
<ol class="arabic">
<li><p>Get the current value of the PCI configure register:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>setpci -s &lt;XX:XX.X&gt; a8.w
</pre></div>
</div>
</li>
<li><p>Set bit 8:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>value = value | 0x100
</pre></div>
</div>
</li>
<li><p>Set the PCI configure register with new value:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>setpci -s &lt;XX:XX.X&gt; a8.w=&lt;value&gt;
</pre></div>
</div>
</li>
</ol>
</section>
<section id="i40e-vlan-strip-of-vf">
<h3><span class="section-number">32.7.3. </span>i40e: Vlan strip of VF</h3>
<p>The VF vlan strip function is only supported in the i40e kernel driver &gt;= 2.1.26.</p>
</section>
<section id="i40e-vlan-filtering-of-vf">
<h3><span class="section-number">32.7.4. </span>i40e: Vlan filtering of VF</h3>
<p>For i40e driver 2.17.15, configuring VLAN filters from the DPDK VF is unsupported.
When applying VLAN filters on the VF it must first be configured from the
corresponding PF.</p>
</section>
<section id="ice-vf-inserts-vlan-tag-incorrectly-on-avx-512-tx-path">
<h3><span class="section-number">32.7.5. </span>ice: VF inserts VLAN tag incorrectly on AVX-512 Tx path</h3>
<p>When the kernel driver requests the VF to use the L2TAG2 field of the Tx context
descriptor to insert the hardware offload VLAN tag,
AVX-512 Tx path cannot handle this case correctly
due to its lack of support for the Tx context descriptor.</p>
<p>The VLAN tag will be inserted to the wrong location (inner of QinQ)
on AVX-512 Tx path.
That is inconsistent with the behavior of PF (outer of QinQ).
The ice kernel driver version newer than 1.8.9 requests to use L2TAG2
and has this issue.</p>
<p>Set the parameter <cite>–force-max-simd-bitwidth</cite> as 64/128/256
to avoid selecting AVX-512 Tx path.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/DPDK_logo_vertical_rev_small.png" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="../index.html">Data Plane Development Kit</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../linux_gsg/index.html">Getting Started Guide for Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../freebsd_gsg/index.html">Getting Started Guide for FreeBSD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../windows_gsg/index.html">Getting Started Guide for Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sample_app_ug/index.html">Sample Applications User Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prog_guide/index.html">Programmer’s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto/index.html">HowTo Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tools/index.html">DPDK Tools User Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../testpmd_app_ug/index.html">Testpmd Application User Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Network Interface Controller Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bbdevs/index.html">Baseband Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cryptodevs/index.html">Crypto Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compressdevs/index.html">Compression Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vdpadevs/index.html">vDPA Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regexdevs/index.html">REGEX Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mldevs/index.html">Machine Learning Device Driver</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dmadevs/index.html">DMA Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpus/index.html">General-Purpose Graphics Processing Unit Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../eventdevs/index.html">Event Device Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rawdevs/index.html">Rawdev Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mempool/index.html">Mempool Device Driver</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platform/index.html">Platform Specific Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributor’s Guidelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rel_notes/index.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Network Interface Controller Drivers</a><ul>
      <li>Previous: <a href="igc.html" title="previous chapter"><span class="section-number">31. </span>IGC Poll Mode Driver</a></li>
      <li>Next: <a href="ionic.html" title="next chapter"><span class="section-number">33. </span>IONIC Driver</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      
      
      
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.3.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/nics/intel_vf.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>